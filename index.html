<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Carlos Anzola | Robotics Engineer</title>
<link rel="stylesheet" href="style.css">
<link rel="stylesheet" href="style.css?v=1.1">  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>

<nav>
  <div class="nav-content">
    <span class="nav-logo">C. Anzola</span>
    <div class="nav-links">
      <a href="#education">Education</a>
      <!-- <a href="#skills">Skills</a> -->
      <a href="#portfolio">Portfolio</a>
    </div>
  </div>
</nav>

<main>

  <section id="hero">
    <div class="hero-header">
      <h1>Carlos Anzola</h1>
      <span class="status-badge">M.S. Robotics @ Northeastern University</span>
    </div>
    
    <p class="hero-focus">
      Mechatronics engineer focused on designing and building robotic systems from scratch, 
      with experience across <strong>mechanical</strong>, <strong>electrical</strong>, and <strong>software</strong> domains, 
      and an emphasis on <strong>sensing</strong> and <strong>perception</strong>.
    </p>

    <div class="contact-bar">
      <a href="mailto:anzolacastellanos.c@northeastern.edu" class="btn btn-primary">Email Me</a>
      <a href="https://github.com/cja798" class="btn btn-text">GitHub ↗</a>
      <a href="https://www.linkedin.com/in/carlos-anzola/" class="btn btn-text">LinkedIn ↗</a>
      <a href="legacy/" class="btn btn-text">Legacy Portfolio ↗</a>
    </div>
  </section>

  <hr class="divider">

  <section id="education">
    <h2>Education</h2>
    <div class="grid-2-col">
      <div class="edu-card">
        <h3>M.S. Robotics - Computer Science Concentration</h3>
        <span class="school">Northeastern University | Boston, MA</span>
        <span class="date">Ongoing</span>
        <p class="coursework"><strong>Focus:</strong> Autonomous Field Robotics, Mobile Robotics, Robot Sensing & Navigation,
             Pattern Recognition, Image Processing & Computer Vision, Reinforcement Learning.</p>
      </div>
      <div class="edu-card">
        <h3>B.S. Mechatronics Engineering</h3>
        <span class="school">UNC & NC State</span>
        <span class="date">Joint Degree Program</span>
        <p class="coursework"><strong>Focus:</strong> Embedded Systems, Control Systems, Digital System Design,
            Mechatronics System Modeling, Robotics and Autonomous Systems, Electric Circuits, Logic Design.</p>
      </div>
    </div>
  </section>
<!--  
  <section id="skills">
    <h2>Technical Competencies</h2>
    <div class="skill-grid">
      <div class="skill-group">
        <h4>Software & Control</h4>
        <div class="tech-stack">
          <span>C++</span> <span>Python</span> <span>ROS1/2</span> <span>MATLAB</span> <span>Linux</span> <span>Git</span> <span>OpenCV</span> <span>TensorFlow</span>
        </div>
      </div>
      <div class="skill-group">
        <h4>Hardware & Mechanical</h4>
        <div class="tech-stack">
          <span>SolidWorks</span> <span>Circuit Design</span> <span>Soldering</span> <span>3D Printing</span> <span>Laser Cutting</span> <span>Waterjet</span>
        </div>
      </div>
    </div>
  </section>
-->
  <hr class="divider">

  <section id="portfolio">
    <h2>Engineering Portfolio</h2>
    <p class="section-intro">Selected academic projects and professional experience.</p>

    <!-- NeuROAM -->
    <article class="portfolio-entry">
      <header class="entry-header">
        <div class="entry-meta">
          <span class="type-tag">Research Project</span>
          <span class="date">Institute for Experiential Robotics: Field Robotics Lab</span>
        </div>
        <h3>NeuROAM: Robotic Observation & Mapping Payload</h3>
        <div class="tech-stack">
          <span>SolidWorks</span> <span>ROS2</span> <span>Rapid Prototyping</span>
          <span>3D Printing</span> <span>Laser Cutting</span>  <span>Waterjet</span> <span>Tapping</span> <span>Drilling</span>
          <span>Soldering</span> <span>Cable Fabrication</span> <span>Circuit Design</span> <span>Documentation</span>
          <span>Mechatronics</span> <span>Cable Harnessing</span> <span>Hardware Integration</span>
        </div>
      </header>

      <div class="entry-content">
        <div class="project-description">
            <p>Designed and built a modular sensor payload for multi-platform data collection, supporting robots such as Boston Dynamics <strong>SPOT</strong>, Unitree <strong>Go2, Go2W, & G1</strong>, and AgileX <strong>Scout & Hunter</strong>.</p>
            <ul class="highlight-list">
                <li>Integrated a high-fidelity sensor suite including <strong>stereo cameras, LiDAR, IMU, GPS, and radio </strong> communication.</li>
                <li>Developed full CAD assemblies in SolidWorks to ensure mechanical compatibility across diverse robots.</li>
                <li>Fabricated custom dampers and mounting hardware to mitigate vibration noise affecting IMU data.</li>
                <li>Executed end-to-end electrical integration, including custom cable fabrication (JST, Dupont, USB, RJ45, barrel jack) with sleeving and heat-shrink finishing.</li>
                <li>Successfully assembled and deployed 8 payloads, performing routine hardware maintenance and field-reliability inspections.</li>
                <li>Documented manufacture, assembly instructions, and maintenance procedures to facilitate future builds and repairs.</li>
                <li>Project repository: <a href="https://github.com/neufieldrobotics/NeuROAM">https://github.com/neufieldrobotics/NeuROAM</a></li>
            </ul>
            <p class="credits">
                <strong>Credits:</strong> Developed in collaboration with <strong>Jasen Levoy</strong> (Design consultant) and <strong>Nikolas Sanderson</strong> (Assembly).
            </p>
        </div>

        <div class="media-gallery">
          <figure>
            <img src="assets/neuroam_payload.gif" alt="NeuROAM Assembly">
            <figcaption>CAD Assembly.</figcaption>
          </figure>
          <figure>
             <img src="assets/neuroam_robots_static.JPG" alt="Robots with NeuROAM payloads mounted ready to collect data">
             <figcaption>Robots with NeuROAM payloads mounted ready to collect data.</figcaption>
          </figure>

          <figure class="full-width">
            <img src="assets/neuroam_team.JPG" alt="Team Photo">
            <figcaption>NeuROAM Team.</figcaption>
          </figure>

          <figure>
            <video autoplay loop muted playsinline controls preload="metadata">
                <source src="assets/neuroam_robots_running.mp4#t=2,18" type="video/mp4">
                Your browser does not support the video tag.
            </video>
            <figcaption>Robots in operation with NeuROAM payloads.</figcaption>
          </figure>
          
          <figure>
             <img src="assets/neuroam_assembled_payloads.JPG" alt="SolidWorks Exploded View">
             <figcaption>Assembled payloads.</figcaption>
          </figure>
          <figure>
             <img src="assets/neuroam_payload_inside.JPG" alt="SolidWorks Exploded View">
             <figcaption>Inside view of the payload showing component layout.</figcaption>
          </figure>

          <figure>
             <img src="assets/neuroam_g1_with_backpack.JPG" alt="SolidWorks Exploded View">
             <figcaption>Unitree G1 humanoid with NeuROAM payload mounted.</figcaption>
          </figure>

          <figure>
             <img src="assets/neuroam_g1_with_payload_back.JPG" alt="SolidWorks Exploded View">
             <figcaption>Rear view of Unitree G1 humanoid with updated payload.</figcaption>
          </figure>
          <figure>
             <img src="assets/neuroam_g1_with_payload_front.JPG" alt="SolidWorks Exploded View">
             <figcaption>Front view of Unitree G1 humanoid with updated payload.</figcaption>
          </figure>

          
        </div>

        <div class="pdf-viewer">
          <h4>Technical Documentation</h4>
          <iframe src="assets/NEUROAM_PAYLOAD_ASSEMBLY_INSTRUCTIONS.pdf" width="100%" height="600px">
          </iframe>
          <p class="pdf-fallback"><em>Note: If the PDF does not load, <a href="assets/NEUROAM_PAYLOAD_ASSEMBLY_INSTRUCTIONS.pdf">click here to view it</a>.</em></p>
        </div>

      </div>
    </article>

    <hr class="entry-divider">

    <!-- COLMAP -->
    <article class="portfolio-entry">
    <header class="entry-header">
      <div class="entry-meta">
        <span class="type-tag">Class Project</span>
        <span class="date">Autonomous Field Robotics</span>
      </div>
      <h3>Machine-Learning-Based COLMAP & PyCOLMAP</h3>
      <div class="tech-stack">
        <span>COLMAP</span> <span>PyCOLMAP</span> <span>SfM</span> <span>C++</span> <span>Python</span>
      </div>
    </header>

    <div class="entry-content">
      <ul class="highlight-list">
          <li>Modified the <strong>C++ source code</strong> to embed <strong>SuperPoint, LightGlue, DISK, and LoFTR</strong> into the native GUI, allowing for high-performance ML-driven matching within the original software interface.</li>
          <li>Developed a Python-based pipeline using PyCOLMAP to implement SuperPoint and SuperGlue for automatic reconstructions.</li>
          <li>Code repository: <a href="https://github.com/CJA798/neucolmap">https://github.com/CJA798/neucolmap</a></li>
      </ul>
      <p class="credits">
          <strong>Credits:</strong> Developed in collaboration with <strong>Srijan Dokania</strong> (LoFTR implementation).
      </p>
      <figure class="full-width"> <img src="assets/colmap_python_comparison.jpg" alt="PyCOLMAP Results Comparison">
        <figcaption>PyCOLMAP Results Comparison.</figcaption>
      </figure>

      <div class="small-grid">
        <figure>
          <img src="assets/colmap_sift_flann.gif" alt="COLMAP SIFT and FLANN Matching">
          <figcaption>COLMAP SIFT and FLANN Matching.</figcaption>
        </figure>
        <figure>
          <img src="assets/colmap_sp_lg.gif" alt="COLMAP SuperPoint and LightGlue Matching">
          <figcaption>COLMAP SuperPoint and LightGlue Matching.</figcaption>
        </figure>
        <figure>
          <img src="assets/colmap_disk_lg.gif" alt="COLMAP DISK and LightGlue Matching">
          <figcaption>COLMAP DISK and LightGlue Matching.</figcaption>
        </figure>
        <figure>
          <img src="assets/colmap_loftr_a.gif" alt="COLMAP LoFTR Matching">
          <figcaption>COLMAP LoFTR Matching.</figcaption>
        </figure>
        <figure>
          <img src="assets/colmap_loftr_b.gif" alt="COLMAP LoFTR Matching and Camera Extrinsics">
          <figcaption>COLMAP LoFTR Matching & Camera Extrinsics.</figcaption>
        </figure>
      </div> </div>
    </article>

    <hr class="entry-divider">

    <!-- IMU-Based Dead Reckoning -->
    <article class="portfolio-entry">
      <header class="entry-header">
         <div class="entry-meta">
          <span class="type-tag">Class Project</span>
          <span class="date">Robotics Sensing and Navigation</span>
        </div>
        <h3>IMU-Based Dead Reckoning</h3>
        <div class="tech-stack">
          <span>Sensor Fusion</span> <span>Python</span> <span>Pandas</span> <span>ROS2</span> <span>Data Visualization</span> <span>Linux</span>
          <span>Inertial Navigation</span> <span>Magnetometer Calibration</span>
        </div>
        <div class="project-description">
            <p>
                I developed a <strong>sensor fusion</strong> pipeline to estimate 2D vehicle trajectories by integrating accelerometer, gyroscope, and magnetometer data. The project focused on overcoming the inherent drift and noise challenges of inertial navigation through rigorous data preprocessing and multi-filter estimation.
            </p>
            <ul class="highlight-list">
                <li>Developed custom ROS2 drivers to synchronize high-frequency IMU data with GPS ground truth, ensuring precise temporal alignment for vehicle dynamics analysis.</li>
                <li>Implemented a <strong>magnetometer calibration</strong> routine that corrected for hard-iron and soft-iron distortions using ellipse fitting and <strong>Isolation Forest</strong> for automated outlier detection.</li>
                <li>Engineered a multi-filter solution for yaw estimation that fused high-frequency gyroscope responsiveness with the long-term stability of the magnetometer, utilizing <strong>Mean Squared Error (MSE) minimization</strong> to calibrate the filter weights.</li>
                <li>GPS driver repository: <a href="https://github.com/CJA798/ROS2-BU-353N-GPS-Driver">https://github.com/CJA798/ROS2-BU-353N-GPS-Driver</a></li>
                <li>IMU driver repository: <a href="https://github.com/CJA798/ROS2-VN100-IMU-AHRS-Driver">https://github.com/CJA798/ROS2-VN100-IMU-AHRS-Driver</a></li>
            </ul>
        </div>
        
        <div class="small-grid">
        <figure>
             <img src="assets/imu_dr_position.png" alt="IMU Dead Reckoning Position Plot">
             <figcaption>IMU Dead Reckoning Position Plot.</figcaption>
        </figure>
        <figure>
             <img src="assets/imu_dr_mag_calibration_xy_comparison.png" alt="IMU Magnetometer Calibration XY Comparison">
             <figcaption>IMU Magnetometer Calibration XY Comparison.</figcaption>
        </figure>
        <figure>
             <img src="assets/imu_dr_velocity_corrected.png" alt="IMU Velocity Corrected Plot">
             <figcaption>IMU Velocity Corrected Plot.</figcaption>
        </figure>
        <figure>
             <img src="assets/imu_dr_yaw_final_comp.png" alt="IMU Yaw Final Comparison">
             <figcaption>IMU Yaw Final Comparison.</figcaption>
        </figure> 
     </div>
      </header>

      <div class="entry-content">

        <div class="pdf-viewer">
          <h4>Technical Report</h4>
          <iframe src="assets/imu_dr_report.pdf" width="100%" height="600px">
          </iframe>
          <p class="pdf-fallback"><em>Note: If the PDF does not load, <a href="assets/imu_dr_report.pdf">click here to view it</a>.</em></p>
        </div>

      </div>
    </article>

    <hr class="entry-divider">

    <!-- Image Mosaicing, Panorama, and Reconstruction + Pose -->
    <article class="portfolio-entry">
      <header class="entry-header">
        <div class="entry-meta">
          <span class="type-tag">Collection of Class Projects</span>
          <span class="date">Autonomous Field Robotics</span>
        </div>
        <h3>Image Mosaicing, Panorama, and Reconstruction w/ Pose Estimation</h3>
        <div class="tech-stack">
          <span>Python</span> <span>OpenCV</span> <span>Structure from Motion</span> <span>Pose Estimation</span>
          <span>Image Processing</span> <span>Computer Vision</span> <span>SfM</span> <span>3D Reconstruction</span>
          <span>Photogrammetry</span>
        </div>
      </header>

      <div class="entry-content">
        <div class="project-description">
            <p>
                Developed multiple computer vision pipelines from scratch to create image mosaics, panoramas, and 3D reconstructions, incorporating pose estimation to recover camera trajectories.
            </p>
            <ul class="highlight-list">
              <li>Developed a complete Image Mosaicing pipeline from scratch, implementing <strong>Homography estimation</strong> and perspective warping to stitch multiple frames into a panorama.</li>
              <li>Implemented the <strong>8-point algorithm</strong> to estimate the Fundamental and Essential matrices, enabling the recovery of epipolar geometry between image pairs.</li>
              <li>Designed a feature matching system using <strong>RANSAC</strong> to filter outliers and <strong>CLAHE</strong> (Contrast Limited Adaptive Histogram Equalization) to improve feature detection in low-contrast regions.</li>
              <li>Created a 3D reconstruction pipeline that performs <strong>Triangulation</strong> of matched points and decomposes the Essential matrix to recover 6-DOF camera pose and trajectory.</li>
              <li>Integrated <strong>Pose Estimation</strong> with covariance analysis to visualize the uncertainty and reliability of the recovered camera path during the reconstruction process.</li>
          </ul>
        </div>
        <div class="media-gallery mosaic-grid">
        <figure>
            <img src="assets/homography_points.png" alt="Homography Points">
            <figcaption>Manual Point Selection.</figcaption>
        </figure>
        <figure>
            <img src="assets/homography_warped.png" alt="Warped Frames">
            <figcaption>Perspective Warping Results.</figcaption>
        </figure>

        <figure class="wide">
            <img src="assets/homography_mosaic.png" alt="Homography Panorama">
            <figcaption>Stitched Homography Panorama.</figcaption>
        </figure>

        <figure>
            <img src="assets/mosaic_intensity_heatmap.png" alt="Intensity Heatmap">
            <figcaption>Intensity Heatmap.</figcaption>
        </figure>
        <figure>
            <img src="assets/mosaic_clahe.png" alt="CLAHE Enhancement">
            <figcaption>CLAHE Enhancement for Feature Detection.</figcaption>
        </figure>
        <figure>
            <img src="assets/mosaic_matcher_comparison.png" alt="Matcher Comparison">
            <figcaption>Keypoint Matching Comparison.</figcaption>
        </figure>
        <figure>
            <img src="assets/mosaic_poses.png" alt="Estimated Trajectory">
            <figcaption>Pose Trajectory & Covariance.</figcaption>
        </figure>

        <figure class="wide">
            <img src="assets/mosaic_final.png" alt="Final Mosaic">
            <figcaption>Final Mosaic.</figcaption>
        </figure>

        <figure>
            <img src="assets/sfm_epipolar_lines.png" alt="Epipolar Lines">
            <figcaption>Epipolar Lines.</figcaption>
        </figure>
        <figure>
            <img src="assets/sfm_reconstruction.png" alt="Buddha Reconstruction">
            <figcaption>3D Point Cloud Reconstruction.</figcaption>
        </figure>
    </div>
    </article>

    <!-- IMU-Noise Characterization & Allan Variance -->
    <article class="portfolio-entry">
      <header class="entry-header">
         <div class="entry-meta">
          <span class="type-tag">Class Project</span>
          <span class="date">Robotics Sensing and Navigation</span>
        </div>
        <h3>IMU Noise Characterization & Allan Variance</h3>
        <div class="tech-stack">
          <span>Python</span> <span>Pandas</span> <span>ROS2</span> <span>Data Visualization</span>
          <span>Stochastic Modeling</span> <span>IMU Error Characterization</span>
        </div>
      </header>

      <div class="entry-content">
        <div class="project-description">
            <p>
                I conducted an in-depth stochastic analysis of a VectorNav VN-100 IMU to characterize its noise profile and stability over time. By analyzing stationary data collected over multiple hours, I modeled the underlying error sources that limit the accuracy of long-term inertial navigation.
            </p>
            <ul class="highlight-list">
                <li>Utilized <strong>Allan Variance (AVAR)</strong> analysis to isolate and quantify specific noise components, including Angle Random Walk, Bias Instability, and Rate Random Walk, for the accelerometer and gyroscope axes.</li>
                <li>Implemented <strong>Power Spectral Density (PSD)</strong> estimations and time-series histograms to validate the white noise assumptions and identify environmental interference or sampling artifacts.</li>
                <li>Validated manufacturer specifications with experimental results.</li>
            </ul>
        </div>

        <div class="pdf-viewer">
          <h4>Technical Report</h4>
          <iframe src="assets/avar_report.pdf" width="100%" height="600px">
          </iframe>
          <p class="pdf-fallback"><em>Note: If the PDF does not load, <a href="assets/avar_report.pdf">click here to view it</a>.</em></p>
        </div>

      </div>
    </article>

    <hr class="entry-divider">

    <!-- IEEE SouthEastCon Design & Hardware Competition -->
    <article class="portfolio-entry">
      <header class="entry-header">
        <div class="entry-meta">
          <span class="type-tag">Competition & Class Project</span>
          <span class="date">IEEE SouthEastCon 2024 & Senior Design </span>
        </div>
        <h3>IEEE SouthEastCon Design & Hardware Competition</h3>
        <div class="tech-stack">
          <span>Linux</span> <span>ROS</span> <span>Raspberry Pi</span> <span>OpenCV</span> <span>Image Processing</span>
          <span>Python</span> <span>C++</span> <span>Finite State Machine</span> <span>Camera Calibration</span> <span>Logging and Debugging</span>
          <span>Autonomous Navigation</span> <span>System Integration</span> <span>State Machine</span>
        </div>
      </header>

      <div class="entry-content">
        <div class="project-description">
            <p>
                Served as the system integration and vision lead for the UNCA robotics team, earning <strong>1st Place</strong> in the design competition and securing the top three scores in the hardware competition.
            </p>
            <ul class="highlight-list">
                <li>Engineered the central control architecture using a <strong>finite state machine</strong> in Python and C++, coordinating interactions between the drive base, a 6-DOF arm, and the vision system through ROS.</li>
                <li>Developed a computer vision pipeline utilizing color segmentation and morphological filtering to identify and isolate target objects under variable lighting conditions.</li>
                <li>Implemented a localization algorithm using contour analysis to calculate 3D pose and position, translating 2D image coordinates into real-world spatial data for the manipulator arm.</li>
                <li>Reduced localization error by applying <strong>polynomial regression</strong> to experimental data, effectively compensating for lens distortion and perspective shifts.</li>
                <li>Conducted camera calibration for a wide-field-of-view lens to eliminate geometric distortion and maintain high-fidelity visual feedback.</li>
            </ul>

            <p class="credits">
                <strong>Team Credits:</strong> <strong>Chris Bass</strong> (CAD, Mechanism Design, 6-DOF Arm, Manufacture), <strong>Max Turcios</strong> (Software Integration), <strong>Zach Price</strong> (PCB Design & Sensor Integration),
                <strong>Alex Haines</strong> & <strong>Jason Robinson</strong> (Misc).
            </p>
        </div>
        <div class="media-gallery">
            <figure>
                <video autoplay loop muted playsinline controls preload="metadata">
                    <source src="assets/IEEE24_bot.mp4#t=2,18" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <figcaption>Robot successfully navigating the course.</figcaption>
            </figure>

            <figure>
                <img src="assets/IEEE24_robot_award.png" alt="IEEE SouthEastCon Robot and Award">
                <figcaption>Robot and 1st Place Award.</figcaption>
            </figure>

            <figure>
                <img src="assets/IEEE24_team.PNG" alt="IEEE SouthEastCon Team Photo">
                <figcaption>UNCA Team.</figcaption>
            </figure>
            <figure>
                <img src="assets/IEEE24_bot.png" alt="CAD Model of Robot">
                <figcaption>CAD Model of Robot.</figcaption>
            </figure>

            <figure>
                <img src="assets/IEEE24_uncalibrated.png" alt="Uncalibrated camera image">
                <figcaption>Uncalibrated camera image.</figcaption>
            </figure>
            <figure>
                <img src="assets/IEEE24_calibrated.png" alt="Calibrated camera image">
                <figcaption>Calibrated camera image.</figcaption>
            </figure>
            <figure>
                <img src="assets/IEEE24_detection.png" alt="Detection and localization of objects">
                <figcaption>Detection and localization of objects.</figcaption>
            </figure>
            
        
        </div>

        <div class="pdf-viewer">
          <h4>Technical Report</h4>
          <iframe src="assets/IEEE24_report.pdf" width="100%" height="600px">
          </iframe>
          <p class="pdf-fallback"><em>Note: If the PDF does not load, <a href="assets/IEEE24_report.pdf">click here to view it</a>.</em></p>
        </div>
    </article>

    <hr class="entry-divider">

    <!-- Camouflaged Cuttlefish Detection -->
    <article class="portfolio-entry">
    <header class="entry-header">
        <div class="entry-meta">
        <span class="type-tag">Class Project</span>
        <span class="date">Image Processing and Pattern Recognition</span>
        </div>
        <h3>Camouflaged Cuttlefish Detection using Classic and Deep Learning Techniques</h3>
        <div class="tech-stack">
        <span>MATLAB</span> <span>Python</span> <span>Image Processing</span> <span>Computer Vision</span> <span>Pattern Recognition</span>
        <span>Image Segmentation</span>
        </div>
    </header>

    <div class="entry-content">
        <div class="project-description">
            <p>
                Evaluated and implemented multiple image segmentation pipelines to detect camouflaged cuttlefish in complex underwater environments, comparing the performance and generalization of classical computer vision against deep learning architectures.
            </p>

            <p class="credits">
                <strong>Credits:</strong> Developed in collaboration with <strong>Li Wang</strong> (U-Net implementation and analysis).
            </p>
        </div>

        <figure class="full-width">
            <img src="assets/cuttlefish_83246588_corrections.png" alt="Enhanced Cuttlefish Image">
            <figcaption>Pre-processing.</figcaption>
        </figure>

        <div class="small-grid">
            <figure>
                <img src="assets/cuttlefish_83246588_cluster_map.png" alt="Cluster Map of Cuttlefish after K-Means Segmentation">
                <figcaption>Cluster map after K-Means segmentation.</figcaption>
            </figure>
            <figure>
                <img src="assets/cuttlefish_83246588_eval.png" alt="Evaluation Metrics for Cuttlefish Detection">
                <figcaption>Evaluation metrics for detection performance.</figcaption>
            </figure>
        </div>

        <figure class="full-width">
            <img src="assets/cuttlefish_83246588_result.png" alt="Detection Result of Cuttlefish">
            <figcaption>Detection result.</figcaption>
        </figure>

        <div class="pdf-viewer">
        <h4>Technical Report</h4>
        <iframe src="assets/cuttlefish_report.pdf" width="100%" height="600px"></iframe>
        <p class="pdf-fallback"><em>Note: If the PDF does not load, <a href="assets/cuttlefish_report.pdf">click here to view it</a>.</em></p>
        </div>
    </div>
    </article>

    <hr class="entry-divider">

        <!-- X650 Drone -->
    <article class="portfolio-entry">
      <header class="entry-header">
         <div class="entry-meta">
          <span class="type-tag">Research Project</span>
          <span class="date">Institute for Experiential Robotics: Field Robotics Lab</span>
        </div>
        <h3>X650 Drone Upgrade (In Progress...)</h3>
        <div class="tech-stack">
          <span>Hardware</span> <span>Lasercutting</span> <span>Waterjet</span> <span>3D Printing</span> <span>Wire Harnessing</span>
        </div>
      </header>

      <div class="entry-content">

        <div class="project-description">
            <p>
                Currently working on upgrading the mechanical and electrical systems of the lab's X650 drone platform to enhance maintainability and performance for outdoor field operations.
            </p>
        </div>

        <div class="media-gallery">
          <figure>
             <img src="assets/drone_prototype.jpg" alt="X650 Drone Platform">
             <figcaption>X650 Drone Platform First Prototype.</figcaption>
          </figure>
          <figure>
             <img src="assets/drone_side.jpg" alt="X650 Drone Side View">
             <figcaption>X650 Drone Side View.</figcaption>
          </figure>

      </div>
    </article>

    <hr class="entry-divider">

    <!-- GPS Performance & Error Analysis -->
    <article class="portfolio-entry">
      <header class="entry-header">
         <div class="entry-meta">
          <span class="type-tag">Class Project</span>
          <span class="date">Robotics Sensing and Navigation</span>
        </div>
        <h3>GPS Performance & Error Analysis</h3>
        <div class="tech-stack">
          <span>Python</span> <span>Pandas</span> <span>Data Visualization</span> <span>Data Analysis</span>
          <span>NMEA Parsing</span> <span>UTM Projection</span>
        </div>
      </header>

      <div class="entry-content">

        <div class="project-description">
            <p>
                Investigated the accuracy and reliability of GPS data under stationary and dynamic conditions, focusing on analyzing and quantifying the error in commercial-grade receivers.
            </p>
        </div>

        <div class="pdf-viewer">
          <h4>Technical Report</h4>
          <iframe src="assets/gps_report.pdf" width="100%" height="600px">
          </iframe>
          <p class="pdf-fallback"><em>Note: If the PDF does not load, <a href="assets/gps_report.pdf">click here to view it</a>.</em></p>
        </div>

      </div>
    </article>

    <hr class="entry-divider">

  </section>

</main>

<footer>
  <p>© 2026 Carlos Anzola.</p>
</footer>

</body>
</html>